<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.23" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.155" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"逐层分解Transformer","image":[""],"dateModified":"2025-07-07T01:58:48.000Z","author":[]}</script><meta property="og:url" content="https://docs.sreio.com/ai/%E9%80%90%E5%B1%82%E5%88%86%E8%A7%A3Transformer/"><meta property="og:site_name" content="Sreio Docs"><meta property="og:title" content="逐层分解Transformer"><meta property="og:description" content="1. 1.Transformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： Transformer 的整体结构，左图Encoder和右图Decoder 可以看 到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和Decoder 都包..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-07-07T01:58:48.000Z"><meta property="article:modified_time" content="2025-07-07T01:58:48.000Z"><link rel="icon" type="image/png" href="/favicon.ico"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6715279698150024" crossorigin="anonymous"></script><title>逐层分解Transformer | Sreio Docs</title><meta name="description" content="1. 1.Transformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： Transformer 的整体结构，左图Encoder和右图Decoder 可以看 到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和Decoder 都包..."><link rel="preload" href="/assets/style-BXtE0-vj.css" as="style"><link rel="stylesheet" href="/assets/style-BXtE0-vj.css"><link rel="modulepreload" href="/assets/app-DDsjqNbb.js"><link rel="modulepreload" href="/assets/index.html-RT6h6X_i.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-d90a7a26><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d5a8d0bc></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-d5a8d0bc> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-d90a7a26 data-v-e98a6132><div class="vp-navbar" vp-navbar data-v-e98a6132 data-v-2c31ea5e><div class="wrapper" data-v-2c31ea5e><div class="container" data-v-2c31ea5e><div class="title" data-v-2c31ea5e><div class="vp-navbar-title has-sidebar" data-v-2c31ea5e data-v-1a4f50af><a class="vp-link link no-icon title" href="/" data-v-1a4f50af><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="/favicon.ico" alt data-v-480e858a><!--]--><!--[--><img class="vp-image light logo" style="" src="/favicon.ico" alt data-v-480e858a><!--]--><!--]--><!--]--><span data-v-1a4f50af>Sreio Docs</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-2c31ea5e><div class="content-body" data-v-2c31ea5e><!--[--><!--]--><div class="vp-navbar-search search" data-v-2c31ea5e><div class="search-wrapper" data-v-97535d1e><!----><div id="local-search" data-v-97535d1e><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-97535d1e><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-2c31ea5e data-v-d43c1732><span id="main-nav-aria-label" class="visually-hidden" data-v-d43c1732>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-d43c1732 data-v-d4acf911><!--[--><!----><span data-v-d4acf911>首页</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-d43c1732 data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-86530b6c><span class="text" data-v-86530b6c><!----><!----><span data-v-86530b6c>编程语言</span><!----><span class="vpi-chevron-down text-icon" data-v-86530b6c></span></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><div class="items" data-v-709dc2b1><!--[--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/golang/" data-v-1ff1855f><!--[--><!----> Golang <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/php/" data-v-1ff1855f><!--[--><!----> PHP <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-d43c1732 data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-86530b6c><span class="text" data-v-86530b6c><!----><!----><span data-v-86530b6c>缓存&数据库</span><!----><span class="vpi-chevron-down text-icon" data-v-86530b6c></span></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><div class="items" data-v-709dc2b1><!--[--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/mysql/" data-v-1ff1855f><!--[--><!----> mysql <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/redis/" data-v-1ff1855f><!--[--><!----> Redis <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/ELK/" data-v-1ff1855f><!--[--><!----> ELK <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/etcd/" data-v-1ff1855f><!--[--><!----> etcd <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/clickhouse/" data-v-1ff1855f><!--[--><!----> clickhouse <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/mongodb/" data-v-1ff1855f><!--[--><!----> Mongodb <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-d43c1732 data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-86530b6c><span class="text" data-v-86530b6c><!----><!----><span data-v-86530b6c>更多</span><!----><span class="vpi-chevron-down text-icon" data-v-86530b6c></span></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><div class="items" data-v-709dc2b1><!--[--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/ai/" data-v-1ff1855f><!--[--><!----> Ai <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/linux/" data-v-1ff1855f><!--[--><!----> Linux <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/queue/" data-v-1ff1855f><!--[--><!----> 消息队列 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/brain/" data-v-1ff1855f><!--[--><!----> 数据结构与算法 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/network/" data-v-1ff1855f><!--[--><!----> 网络协议 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/interview/" data-v-1ff1855f><!--[--><!----> 面试宝典 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/it/" data-v-1ff1855f><!--[--><!----> IT名词介绍 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/vpn/" data-v-1ff1855f><!--[--><!----> VPN <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/%E9%A9%BE%E7%85%A7%E8%80%83%E8%AF%95/" data-v-1ff1855f><!--[--><!----> 驾照考试 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-709dc2b1 data-v-1ff1855f><a class="vp-link link" href="/cpwp/" data-v-1ff1855f><!--[--><!----> 中国程序员容易发音错误的单词 <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/sites-collect/" tabindex="0" data-v-d43c1732 data-v-d4acf911><!--[--><!----><span data-v-d4acf911>网站导航</span><!----><!--]--><!----></a><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-2c31ea5e data-v-a295abf6><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-a295abf6 data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-2c31ea5e data-v-ad52545c data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="https://github.com/sreio/sreio-docs" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-2c31ea5e data-v-652282fd data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-86530b6c><span class="vpi-more-horizontal icon" data-v-86530b6c></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><!----><!--[--><!--[--><!----><div class="group" data-v-652282fd><div class="item appearance" data-v-652282fd><p class="label" data-v-652282fd>外观</p><div class="appearance-action" data-v-652282fd><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-652282fd data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-652282fd><div class="item social-links" data-v-652282fd><div class="vp-social-links social-links-list" data-v-652282fd data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="https://github.com/sreio/sreio-docs" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-2c31ea5e data-v-2b50024d><span class="container" data-v-2b50024d><span class="top" data-v-2b50024d></span><span class="middle" data-v-2b50024d></span><span class="bottom" data-v-2b50024d></span></span></button></div></div></div></div><div class="divider" data-v-2c31ea5e><div class="divider-line" data-v-2c31ea5e></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-d90a7a26 data-v-3944d8e8><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-3944d8e8><span class="vpi-align-left menu-icon" data-v-3944d8e8></span><span class="menu-text" data-v-3944d8e8>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-3944d8e8 data-v-4114a62c><button data-v-4114a62c>返回顶部</button><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-d90a7a26 data-v-95211354><div class="curtain" data-v-95211354></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-95211354><span id="sidebar-aria-label" class="visually-hidden" data-v-95211354> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0 has-active" data-v-473fd05b data-v-12048f0f><!----><div data-v-12048f0f data-v-12048f0f><div class="items" data-v-12048f0f><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/ai/%E9%80%90%E5%B1%82%E5%88%86%E8%A7%A3Transformer/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>逐层分解Transformer</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-d90a7a26 data-v-b2beaca7><div class="vp-doc-container has-sidebar has-aside" data-v-b2beaca7 data-v-23f6ad98><!--[--><!--]--><div class="container" data-v-23f6ad98><div class="aside" vp-outline data-v-23f6ad98><div class="aside-curtain" data-v-23f6ad98></div><div class="aside-container" data-v-23f6ad98><div class="aside-content" data-v-23f6ad98><div class="vp-doc-aside" data-v-23f6ad98 data-v-5976474c><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-5976474c data-v-aa56eba0><div class="content" data-v-aa56eba0><div class="outline-marker" data-v-aa56eba0></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-aa56eba0><span data-v-aa56eba0>此页内容</span><span class="vpi-print icon" data-v-aa56eba0></span></div><ul class="root" data-v-aa56eba0 data-v-3e6b023c><!--[--><!--]--></ul></div></nav><!--[--><!--[--><!--[--><!--[--><div class="aside-nav-wrapper" data-v-d90a7a26 data-v-45e94c4e><a class="vp-link link no-icon link" href="https://github.com/sreio/sreio-docs" target="_blank" rel="noreferrer" data-v-45e94c4e><!--[--><span class="vpi-github-star" data-v-45e94c4e></span><span class="link-text" data-v-45e94c4e>在 GitHub 上 Star</span><span class="vpi-arrow-right" data-v-45e94c4e></span><!--]--><!----></a><a class="vp-link link no-icon link" href="https://github.com/sreio/sreio-docs/issues/new/choose" target="_blank" rel="noreferrer" data-v-45e94c4e><!--[--><span class="vpi-github-issue" data-v-45e94c4e></span><span class="link-text" data-v-45e94c4e>遇到问题？</span><span class="vpi-arrow-right" data-v-45e94c4e></span><!--]--><!----></a></div><!--]--><!--]--><!--]--><!--]--><div class="spacer" data-v-5976474c></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-23f6ad98><div class="content-container" data-v-23f6ad98><!--[--><!--]--><main class="main" data-v-23f6ad98><nav class="vp-breadcrumb" data-v-23f6ad98 data-v-1ae4ad7a><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-1ae4ad7a><!--[--><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-1ae4ad7a><!--[-->首页<!--]--><!----></a><span class="vpi-chevron-right" data-v-1ae4ad7a></span><meta property="name" content="首页" data-v-1ae4ad7a><meta property="position" content="1" data-v-1ae4ad7a></li><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link link breadcrumb current" href="/ai/%E9%80%90%E5%B1%82%E5%88%86%E8%A7%A3Transformer/" property="item" typeof="WebPage" data-v-1ae4ad7a><!--[-->逐层分解Transformer<!--]--><!----></a><!----><meta property="name" content="逐层分解Transformer" data-v-1ae4ad7a><meta property="position" content="2" data-v-1ae4ad7a></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-27be53cb>逐层分解Transformer <!----></h1><div class="vp-doc-meta" data-v-27be53cb><!--[--><!--]--><p class="reading-time" data-v-27be53cb><span class="vpi-books icon" data-v-27be53cb></span><span data-v-27be53cb>约 3570 字</span><span data-v-27be53cb>大约 12 分钟</span></p><!----><!--[--><!--]--><p class="create-time" data-v-27be53cb><span class="vpi-clock icon" data-v-27be53cb></span><span data-v-27be53cb>2025-07-07</span></p></div><!--]--><!--[--><!--]--><div class="_ai_%E9%80%90%E5%B1%82%E5%88%86%E8%A7%A3Transformer_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-23f6ad98><!--[--><!--]--><div data-v-23f6ad98><h2 id="_1-1-transformer-整体结构" tabindex="-1"><a class="header-anchor" href="#_1-1-transformer-整体结构"><span>1. 1.Transformer 整体结构</span></a></h2><p>首先介绍<strong> Transformer</strong> 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：</p><figure><img src="/assets/HprpbGjJhosaGnxY2S5cNl2Ynvc-DB1HDJEU.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer 的整体结构，左图Encoder和右图Decoder</p><div class="hint-container note"><p class="hint-container-title">可以看 到<strong> Transformer </strong>由<strong><code>Encoder</code></strong> 和<strong><code>Decoder</code></strong>两个部分组成，<strong><code>Encoder</code></strong> 和<strong><code>Decoder</code></strong> 都包含<strong> 6 个 <code>block</code>。</strong></p></div><ul><li><p><strong>Transformer </strong>的工作流程大体如下：</p><ul><li><strong>第一步</strong>：获取输入句子的每一个单词的表示<strong>向量 X</strong>，X由单词的<strong><code> Embedding</code></strong>（Embedding就是从原始数据提取出来的<strong><code>Feature</code></strong>） 和单词位置的<strong><code> Embedding</code></strong> 相加得到。</li></ul></li></ul><figure><img src="/assets/DOL2b5yGQooVgQxIwKAcsuwsnDs-CI1kruMH.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer 的输入表示</p><figure><img src="/assets/Yga5bKDucoCAI6xMV7ucwq8HnQc-Bh9MyLkW.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transofrmer Decoder 预测</p><div class="hint-container note"><p class="hint-container-title">上图<strong><code>Decoder</code></strong>接收了<strong><code> Encoder </code></strong>的<strong>编码矩阵 C</strong>，然后首先输入一个翻译开始符 <strong><code>&quot;&lt;Begin&gt;&quot;</code></strong>，预测第一个单词 <strong><code>&quot;I&quot;</code></strong>；然后输入翻译开始符<code></code><strong><code>&quot;&lt;Begin&gt;&quot;</code></strong> 和单词<strong><code> &quot;I&quot;</code></strong>，预测单词<strong><code> &quot;have&quot;</code></strong>，以此类推。这是 <strong>Transformer</strong> 使用时候的大致流程，接下来是里面各个部分的细节。</p></div><h2 id="_2-transformer-的输入" tabindex="-1"><a class="header-anchor" href="#_2-transformer-的输入"><span>2. Transformer 的输入</span></a></h2><div class="hint-container note"><p class="hint-container-title"><strong>Transformer</strong> 中单词的输入表示<strong> x</strong>由<strong><code>单词 Embedding</code></strong> 和<strong><code>位置 Embedding </code></strong> <strong>（<code>Positional Encoding</code>）</strong>相加得到。</p></div><figure><img src="/assets/RMfOb6oRboZGV8xtkGoc4BlHnNh-CaIArZFk.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer 的输入表示</p><h3 id="_2-1-单词-embedding" tabindex="-1"><a class="header-anchor" href="#_2-1-单词-embedding"><span>2.1 单词 <code>Embedding</code></span></a></h3><p><strong><code>单词的 Embedding</code></strong>有很多种方式可以获取，例如可以采用 <strong>Word2Vec</strong>、<strong>Glove </strong>等算法预训练得到，也可以在 <strong>Transformer</strong> 中训练得到。</p><h3 id="_2-2-位置-embedding" tabindex="-1"><a class="header-anchor" href="#_2-2-位置-embedding"><span>2.2 位置 <code>Embedding</code></span></a></h3><div class="hint-container note"><p class="hint-container-title"><strong><code>Transformer</code></strong> 中除了单词的<strong> Embedding</strong>，还需要使用位置 Embedding 表示单词出现在句子中的位置。因为 Transformer <strong><code>不采用 RNN 的结构</code></strong>，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 <strong>NLP</strong> 来说非常重要。所以 <strong>Transformer</strong> 中使用位置 <strong>Embedding</strong> 保存单词在序列中的相对或绝对位置。</p></div><p>位置 Embedding 用<strong> PE</strong>表示，<strong>PE</strong> 的维度与单词 <strong><code>Embedding</code></strong> 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者</p><p><strong>计算公式如下：</strong></p><figure><img src="/assets/Ugq7bu6yIooIjyxRw6ccfYbbnTf-zVnutV2d.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>其中，<strong><code>pos 表示单词在句子中的位置</code></strong>，<strong><code>d 表示 PE的维度 (与词 Embedding 一样</code>)</strong>，<strong><code>2i 表示偶数的维度</code></strong>，<strong><code>2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)</code></strong>。使用这种公式计算 PE 有以下的好处：</p><div class="hint-container note"><p class="hint-container-title">注</p><ul><li>使<strong> PE</strong> 能够适应比<strong>训练集</strong>里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的<strong> Embedding</strong>。</li><li><strong>可以让模型容易地计算出相对位置</strong>，对于固定长度的间距<strong> k，PE(pos+k) </strong>可以用 <strong>PE(pos) </strong>计算得到。</li><li>因为 <strong><code>Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)</code></strong>。</li></ul></div><h2 id="_3-self-attention-自注意力机制" tabindex="-1"><a class="header-anchor" href="#_3-self-attention-自注意力机制"><span>3. Self-Attention（自注意力机制）</span></a></h2><p><strong> Transformer 的内部结构图</strong></p><ul><li><p>左侧为 <strong>Encoder block</strong></p></li><li><p>右侧为 <strong>Decoder block</strong></p></li><li><p>红色圈中的部分为 <strong><code>Multi-Head Attention</code></strong>，是由多个 <strong><code>Self-Attention</code></strong>组成的</p></li></ul><ol><li><p>可以看到 <strong>Encoder block </strong>包含一个 <code>Multi-Head Attention</code></p></li><li><p>而 <strong>Decoder block </strong>包含两个<strong><code> Multi-Head Attentio</code>n </strong>(其中有一个用到 Masked)。</p></li><li><p><strong><code>Multi-Head Attention</code></strong>上方还包括一个<strong> Add &amp; Norm 层</strong></p></li><li><p><strong>Add </strong>表示残差连接<strong><code> (Residual Connection)</code></strong> 用于防止网络退化，</p></li><li><p><strong>Norm</strong> 表示 <strong><code>Layer Normalization</code></strong>，用于对每一层的激活值进行<strong>归一化</strong>。</p></li></ol><figure><img src="/assets/FaCTb171Iohpzsxm3l1c5Fmlnmb-BzEHPWaz.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>因为 <strong>Self-Attention</strong>是 Transformer 的重点，所以我们重点关注 <strong><code>Multi-Head Attention</code></strong> 以及<strong><code> Self-Attention</code></strong>，首先详细了解一下<strong><code> Self-Attention </code></strong>的内部逻辑。</p><h3 id="_3-1-self-attention-结构" tabindex="-1"><a class="header-anchor" href="#_3-1-self-attention-结构"><span>3.1 <code>Self-Attention 结构</code></span></a></h3><figure><img src="/assets/1280X1280(1)-CJL928K_.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Self-Attention 结构</p><p>上图是 <strong><code>Self-Attention</code></strong>的结构，在计算的时候需要用到矩阵<strong>Q(查询),K(键值)</strong>,<strong>V(值)</strong>。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而<strong>Q,K,V</strong>正是通过 <strong><code>Self-Attention</code></strong> 的输入进行<strong>线性变换</strong>得到的。</p><h3 id="_3-2-q-k-v-的计算" tabindex="-1"><a class="header-anchor" href="#_3-2-q-k-v-的计算"><span>3.2 <strong><code>Q, K, V 的计算</code></strong></span></a></h3><div class="hint-container note"><p class="hint-container-title"><strong><code>Self-Attention</code></strong>的输入用<strong>矩阵X</strong>进行表示，则可以使用<strong>线性变阵矩阵WQ</strong>,<strong>WK</strong>,<strong>WV</strong>计算得到<strong>Q</strong>,<strong>K</strong>,<strong>V</strong>。计算如下图所示，注意 X, Q, K, V 的每一行都表示一个单词。</p></div><figure><img src="/assets/1280X1280%20(1)(1)-C_Kd0hLO.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Q, K, V 的计算</p><h3 id="实现" tabindex="-1"><a class="header-anchor" href="#实现"><span><strong>实现</strong></span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># self-attention 实现</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> numpy </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">as</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> np</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> math </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">class</span><span style="--shiki-light:#2E8F82;--shiki-dark:#5DA994;"> Self_Attention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">Module</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # input : batch_size * seq_len * input_dim</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # q : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # k : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # v : batch_size * input_dim * dim_v</span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        super</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Self_Attention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">__init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">_norm_fact </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 1</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> /</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> forward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        Q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # Q: batch_size * seq_len * dim_k</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        K </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # K: batch_size * seq_len * dim_k</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        V </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # V: batch_size * seq_len * dim_v</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">        # Q * K.T() # batch_size * seq_len * seq_len</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        atten </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Softmax</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">            dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">bmm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> K</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">permute</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)))</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> *</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">_norm_fact</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">        # Q * K.T() * V # batch_size * seq_len * dim_v</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        output </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">bmm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> V</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        return</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> output</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">randn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self_atten </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> Self_Attention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 5</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # input_dim:2, k_dim:4, v_dim:5</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">res </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> self_atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">res</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # [4,3,5]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-3-self-attention-的输出" tabindex="-1"><a class="header-anchor" href="#_3-3-self-attention-的输出"><span>3.3 <code> Self-Attention 的输出</code></span></a></h3><p>得到矩阵 <strong>Q</strong>, <strong>K</strong>, <strong>V</strong>之后就可以计算出<strong><code> Self-Attention </code></strong>的输出了，计算的公式如下：</p><figure><img src="/assets/VBRIbrQegofXkxxRhficadiRnTg-DZqzcwct.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Self-Attention 的输出</p><div class="hint-container note"><p class="hint-container-title">公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以 $$d_k$$的平方根。<strong>Q乘以K</strong>的转置后，得到的矩阵行列数都为 <strong>n</strong>，<strong>n </strong>为句子单词数，这个矩阵可以表示单词之间的<strong> attention</strong> 强度。下图为Q乘以$$K^{T}$$ ，<strong>1234</strong> 表示的是句子中的单词。</p></div><figure><img src="/assets/Vci2bIikDoaIYZxdaoVcaYrAn7q-RoXHcxCV.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Q乘以K的转置的计算</p><p>得到$$QK^{T}$$之后，使用 <strong>Softmax</strong> 计算每一个单词对于其他单词的 <strong>attention</strong> 系数，公式中的 <strong><code>Softmax</code></strong>是对矩阵的每一行进行 <strong><code>Softmax</code></strong>，即每一行的和都变为 1.</p><figure><img src="/assets/O68AbVLXAoSIqmxkUoUco5hrnii-DuM1polJ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>对矩阵的每一行进行 Softmax</p><p>得到 <strong><code>Softmax</code></strong> 矩阵之后可以和V相乘，得到最终的输出Z。</p><figure><img src="data:image/webp;base64,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" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Self-Attention 输出</p><p>上图中<strong><code>Softmax</code></strong>矩阵的第 1 行表示单词 1 与其他所有单词的<strong> attention </strong>系数，最终单词 1 的输出 $$Z_1$$ 等于所有单词 i 的值 $$V_i$$ 根据 <strong>attention</strong> 系数的比例加在一起得到，如下图所示：</p><figure><img src="/assets/DHoQb8wuvoXmcBxzdlxcWcPwnuQ-xb7w3FgD.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Zi 的计算方法</p><h3 id="_3-4-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_3-4-multi-head-attention"><span>3.4 <strong><code>Multi-Head Attention</code></strong></span></a></h3><p>在上一步，我们已经知道怎么通过 <strong><code>Self-Attention</code>计算得到输出矩阵 Z</strong>，而 <strong><code>Multi-Head Attention</code></strong> 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的<strong>结构图</strong>。</p><figure><img src="/assets/GmKObFtXqoEKAaxheokctaL1nte-DDU4k354.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Multi-Head Attention</p><p>从上图可以看到 <strong><code>Multi-Head Attention</code></strong> 包含多个<strong> Self-Attention</strong> 层，首先将输入<strong>X</strong>分别传递到 <strong>h</strong> 个不同的 Self-Attention 中，计算得到 <strong>h </strong>个输出矩阵<strong>Z</strong>。下图是<strong> h=8 </strong>时候的情况，此时会得到 8 个输出<strong>矩阵Z</strong>。</p><h3 id="实现-1" tabindex="-1"><a class="header-anchor" href="#实现-1"><span>实现</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">#%%</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># Muti-head Attention 机制的实现</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> math </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nn </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">as</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">class</span><span style="--shiki-light:#2E8F82;--shiki-dark:#5DA994;"> Self_Attention_Muti_Head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">Module</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # input : batch_size * seq_len * input_dim</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # q : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # k : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # v : batch_size * input_dim * dim_v</span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        super</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Self_Attention_Muti_Head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">__init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        assert</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">%</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nums_head </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">==</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 0</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        assert</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">%</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nums_head </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">==</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 0</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nums_head</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">_norm_fact </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 1</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> /</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    </span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> forward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        Q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">//</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        K </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">//</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        V </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">//</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">size</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">())</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        atten </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Softmax</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">matmul</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">K</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">permute</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)))</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> # Q * K.T() # batch_size * seq_len * seq_len</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        output </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">matmul</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">V</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> # Q * K.T() * V # batch_size * seq_len * dim_v</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        return</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> output</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">rand</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># %%</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Self_Attention_Muti_Head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">y</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">y</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># %%</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="/assets/ONKGbFPtNom1GBxbKFYce1KFnLb-YaFb4ELl.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>多个 Self-Attention</p><p>得到 <strong>8 个输出矩阵</strong> $$Z_1$$ 到 $$Z_8$$之后，<strong><code>Multi-Head Attention</code></strong>将它们拼接在一起<strong> (Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出Z。</p><figure><img src="/assets/Xq5zbZPSwoxAfUxZ3mTcK5kAnfe-D6DibkVL.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Multi-Head Attention 的输出</p><p>可以看到 <strong><code>Multi-Head Attention</code></strong> 输出的<strong>矩阵Z</strong>与其输入的<strong>矩阵X</strong>的维度是一样的。</p><h2 id="_4-encoder-结构" tabindex="-1"><a class="header-anchor" href="#_4-encoder-结构"><span>4. Encoder 结构</span></a></h2><p><strong>红色部分</strong>是 Transformer 的<strong><code> Encoder block</code></strong> 结构，可以看到是由 <strong><code>Multi-Head Attention</code></strong>, <strong><code>Add &amp; Norm</code></strong>, <strong><code>Feed Forward</code></strong>, <strong>Add &amp; Norm</strong> 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p><figure><img src="/assets/download_image(1)-Be1p9Feb.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer Encoder block</p><h3 id="_4-1-add-norm" tabindex="-1"><a class="header-anchor" href="#_4-1-add-norm"><span>4.1 Add &amp; Norm</span></a></h3><p><strong>Add &amp; Norm </strong>层由 Add 和 Norm 两部分组成，其计算公式如下：</p><figure><img src="/assets/M9oeblOUkokCYHxuzopczY83n58-GdGlHkaz.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Add &amp;amp;amp;amp;amp; Norm 公式</p><p>其中<strong> X</strong>表示 <strong><code>Multi-Head Attention</code></strong><code></code>或者 <strong>Feed Forward</strong> 的输入，<strong><code>MultiHeadAttention(X</code></strong>) 和 <strong><code>FeedForward(X) </code></strong>表示输出 (输出与输入<strong> X </strong>维度是一样的，所以可以相加)</p><p><strong>Add</strong>指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决<strong>多层网络训练</strong>的问题，可以让网络只关注当前差异的部分，在 <strong>ResNe</strong>t 中经常用到：</p><figure><img src="/assets/HEvpbodcEoUF29xJfipcPbS0nXg-32ctoBQp.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>残差连接</p><p><strong>Norm</strong>指 <strong><code>Layer Normalization</code></strong>，通常用于 <strong>RNN</strong> 结构，<strong><code>Layer Normalization</code></strong> 会将每一层<strong>神经元</strong>的输入都转成<strong>均值方差</strong>都一样的，这样可以加快<strong>收敛</strong>。</p><h3 id="_4-2-feed-forward" tabindex="-1"><a class="header-anchor" href="#_4-2-feed-forward"><span>4.2 Feed Forward</span></a></h3><p><strong>Feed Forward </strong>层比较简单，是一个两层的<strong>全连接层</strong>，第一层的<strong>激活函数</strong>为<strong><code> Relu</code></strong>，第二层不使用激活函数，对应的公式如下。</p><figure><img src="/assets/YNDbb8SBUodwT7xYrdIcUxJVnbQ-DsrRBq70.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Feed Forward</p><p><strong>X</strong>是输入，<strong><code>Feed Forward </code></strong>最终得到的输出矩阵的维度与X一致。</p><h3 id="_4-3-组成-encoder" tabindex="-1"><a class="header-anchor" href="#_4-3-组成-encoder"><span>4.3 组成 Encoder</span></a></h3><div class="hint-container note"><p class="hint-container-title">通过上面描述的<strong><code> Multi-Head Attention</code>,<code> Feed Forward</code></strong><code>,</code><strong><code> Add &amp; Norm</code></strong> 就可以构造出一个 <strong>Encoder block</strong>，<strong>Encoder block </strong>接收输入矩阵 $$X_{(n\times d)}$$ ，并输出一个矩阵$$O_{(n\times d)}$$ 。通过多个 Encoder block 叠加就可以组成 Encoder。</p></div><ul><li><p>第一个 <strong><code>Encoder block </code></strong>的输入为句子单词的表示<strong>向量矩阵</strong>，</p></li><li><p>后续<strong><code>Encoder block</code></strong> 的输入是前一个 Encoder block 的<strong>输出</strong></p></li><li><p>最后一个 <strong><code>Encoder block</code></strong> 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到<strong> Decoder</strong> 中。</p></li></ul><figure><img src="/assets/M4H4bkd4noGXduxQ2c9cHJB9n8g-DLNP4pmN.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-decoder-结构" tabindex="-1"><a class="header-anchor" href="#_5-decoder-结构"><span>5. Decoder 结构</span></a></h2><p><strong>红色部分</strong>为 <strong>Transformer</strong> 的<strong> Decoder block </strong>结构，与 Encoder block 相似，但是存在一些区别：</p><ul><li><p>包含两个<strong><code> Multi-Head Attention</code></strong> 层。</p></li><li><p>第一个 <strong><code>Multi-Head Attention</code></strong> 层采用了 <strong>Masked</strong> 操作。</p></li><li><p>第二个<strong><code>Multi-Head Attention</code></strong> 层的<strong>K</strong>, <strong>V</strong>矩阵使用 <strong>Encoder</strong> 的编码信息矩阵C进行计算，而Q使用上一个 <strong>Decoder block</strong> 的输出计算。</p></li><li><p>最后有一个 <strong><code>Softmax</code></strong> 层计算下一个翻译单词的概率。</p></li></ul><figure><img src="/assets/N6rHbkOcGoSTSMx3WascJbGwnLh-BL8kic17.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer Decoder block</p><h3 id="_5-1-第一个-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_5-1-第一个-multi-head-attention"><span>5.1 <code>第一个 Multi-Head Attention</code></span></a></h3><div class="hint-container note"><p class="hint-container-title"><strong><code>Decoder block</code></strong> 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 &quot;我有一只猫&quot; 翻译成 &quot;I have a cat&quot; 为例，了解一下 Masked 操作。</p></div><ul><li>在<strong> Decoder</strong> 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 &quot;&lt;Begin&gt;&quot; 预测出第一个单词为<strong> &quot;I&quot;</strong>，然后根据输入<strong><code> &quot;&lt;Begin&gt; I</code>&quot;</strong> 预测下一个单词 <strong>&quot;<code>have&quot;</code>。</strong></li></ul><figure><img src="/assets/D6GtbKsPloRCChx9fL1ccWi9n1e-BrpqD8Fr.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Decoder 预测</p><ul><li><p><strong>Decoder </strong>可以在训练的过程中使用 <strong><code>Teacher Forcing</code></strong> 并且<strong>并行化训练</strong>，即将正确的单词序列<strong><code> (&lt;Begin&gt; I have a cat) </code></strong>和对应输出<strong><code> (I have a cat &lt;end&gt;)</code></strong>传递到<strong> Decoder</strong>。那么在预测第 <strong><code>i </code></strong>个输出时，就要将第<strong><code> i+1</code></strong> 之后的单词掩盖住，</p></li><li><p>注意 <strong>Mask</strong> 操作是在 <strong><code>Self-Attention</code></strong> 的<strong><code> Softmax</code></strong>之前使用的，下面用 0 1 2 3 4 5 分别表示<strong><code> &lt;Begin&gt; I have a cat &lt;end&gt;</code></strong>。</p></li></ul><p><strong>第一步</strong>：</p><div class="hint-container note"><p class="hint-container-title">1. 是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 &quot;&lt;Begin&gt; I have a cat&quot; (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p></div><figure><img src="/assets/L0vUbOGLLosQaGxmkZScsL8gnTc-CVQQzVVp.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>输入矩阵与 Mask 矩阵</p><p><strong>第二步</strong>：</p><div class="hint-container note"><p class="hint-container-title">2. &#39;接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算Q和 $$K^T\text{ 的乘积 }QK^T$$</p></div><figure><img src="/assets/DEoobR3MKokqrnx0z3pcfi8CnGe-BZ5unkm7.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Q乘以K的转置</p><p><strong>第三步</strong>：</p><div class="hint-container note"><p class="hint-container-title">3. 在得到 $$QK^{T}$$之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p></div><figure><img src="/assets/GQvAb8XsxoRugTxojv1cMIsdnQg-C-rRhgx3.webp" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Softmax 之前 Mask</p><p>得到 Mask $$QK^{T}$$ 之后在 Mask $$QK^{T}$$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p><p><strong>第四步</strong>：</p><div class="hint-container note"><p class="hint-container-title">4. 使用 Mask $$QK^{T}$$与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 $$Z_1$$ 是只包含单词 1 信息的。</p></div><p><strong>第五步</strong>：</p><div class="hint-container note"><p class="hint-container-title">5. 通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $$Z_i$$ ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出$$Z_i$$ 然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。</p></div><h3 id="_5-2-第二个-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_5-2-第二个-multi-head-attention"><span>5.2 <code> 第二个 Multi-Head Attention</code></span></a></h3><ul><li><p><strong><code>Decoder block</code></strong> 第二个 <strong><code>Multi-Head Attention</code></strong> 变化不大， 主要的区别在于其中 <strong><code>Self-Attention </code></strong>的 <strong>K</strong>, <strong>V</strong>矩阵不是使用 上一个<strong><code>Decoder block</code></strong> 的输出计算的，而是使用<strong> Encoder</strong> 的编码信息矩阵 C 计算的。</p></li><li><p>根据 <strong>Encoder</strong> 的输出 <strong>C</strong>计算得到 <strong>K</strong>, <strong>V</strong>，根据上一个<strong><code> Decoder block</code></strong> 的输出<strong> Z </strong>计算 <strong>Q </strong>(如果是第一个<strong><code>Decoder block</code></strong><code></code>则使用输入<strong>矩阵 X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p></li><li><p>这样做的好处是在 <strong>Decoder </strong>的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p></li></ul><h3 id="_5-3-softmax预测输出单词" tabindex="-1"><a class="header-anchor" href="#_5-3-softmax预测输出单词"><span>5.3 <strong><code>Softmax</code></strong>预测输出单词</span></a></h3><p><strong>Decoder block </strong>最后的部分是利用 <strong><code>Softmax</code></strong> 预测下一个单词，在之前的<strong>网络层</strong>我们可以得到一个最终的输出 Z，因为 <strong>Mask</strong> 的存在，使得单词 <strong><code>0</code></strong>的输出 <strong><code>Z0</code></strong> 只包含单词 <strong><code>0</code></strong>的信息，如下：</p><figure><img src="/assets/image-D_DV6b4u.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Decoder Softmax 之前的 Z</p><p><strong><code>Softmax</code></strong> 根据输出矩阵的每一行预测下一个单词：</p><figure><img src="/assets/image-1-BWqgH4hE.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Decoder Softmax 预测</p><p>这就是 <strong><code>Decoder block</code></strong> 的定义，与<strong><code>Encoder</code></strong>一样，<strong><code>Decoder</code></strong> 是由多个<strong><code> Decoder block</code></strong> 组合而成。</p><h2 id="_6-transformer-总结" tabindex="-1"><a class="header-anchor" href="#_6-transformer-总结"><span>6. Transformer 总结</span></a></h2><div class="hint-container tip"><p class="hint-container-title">提示</p><ul><li>Transformer 与 RNN 不同，可以比较好地并行训练。</li><li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li><li>Transformer 的重点是 Self-Attention 结构，其中用到的 Q, K, V矩阵通过输出进行线性变换得到。</li><li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li></ul></div></div><!--[--><h2 id="doc-contributors" tabindex="-1"><a href="#doc-contributors" class="header-anchor"><span>贡献者</span></a></h2><div class="vp-contributors"><a href="https://github.com/sreio" target="_blank" rel="noreferrer" class="vp-contributor"><img src="https://avatars.githubusercontent.com/sreio?v=4" alt class="vp-contributor-avatar"><span class="vp-contributor-name">sreio</span></a></div><!--]--><!--[--><h2 id="doc-changelog" tabindex="-1"><a href="#doc-changelog" class="header-anchor"><span>更新日志</span></a></h2><div class="vp-changelog-wrapper"><div class="vp-changelog-header"><div class="vp-latest-updated"><span class="vp-changelog-icon"></span><span data-allow-mismatch>2025/7/7 01:58</span></div><div><span class="vp-changelog-menu-icon"></span><span>查看所有更新日志</span></div></div><ul class="vp-changelog-list"><!--[--><li class="vp-changelog-item-commit"><a class="vp-changelog-hash" href="https://github.com/sreio/sreio-docs/commit/61046e09f4c875c19d26a59c6758496fd3f8b2c0" target="_blank" rel="noreferrer"><code>61046</code></a><span class="vp-changelog-divider">-</span><span class="vp-changelog-message">ai &#x26; sites</span><span class="vp-changelog-date" data-allow-mismatch>于 <time datetime="2025-07-07T01:58:48.000Z">2025/7/7</time></span></li><!--]--></ul></div><!--]--><!----></div></main><footer class="vp-doc-footer" data-v-23f6ad98 data-v-fda6bbae><!--[--><!--]--><div class="edit-info" data-v-fda6bbae><div class="edit-link" data-v-fda6bbae><a class="vp-link link no-icon edit-link-button" href="https://github.com/sreio/sreio-docs/edit/main/docs/ai/1.逐层分解Transformer.md" target="_blank" rel="noreferrer" data-v-fda6bbae><!--[--><span class="vpi-square-pen edit-link-icon" aria-label="edit icon" data-v-fda6bbae></span> 编辑此页<!--]--><!----></a></div><!----></div><!----><!----></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-d90a7a26 data-v-bcf8d9a6><span class="percent" data-allow-mismatch data-v-bcf8d9a6>0%</span><span class="show icon vpi-back-to-top" data-v-bcf8d9a6></span><svg aria-hidden="true" data-v-bcf8d9a6><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-bcf8d9a6></circle></svg></button><footer class="vp-footer has-sidebar" vp-footer data-v-d90a7a26 data-v-400675cf><!--[--><div class="container" data-v-400675cf><p class="message" data-v-400675cf>Sreio Docs </p><!----></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-DDsjqNbb.js" defer></script></body></html>