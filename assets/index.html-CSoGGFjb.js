import{_ as i,c as n,e as a,o as t}from"./app-DWByWgeb.js";const e="/assets/HprpbGjJhosaGnxY2S5cNl2Ynvc-DB1HDJEU.webp",h="/assets/DOL2b5yGQooVgQxIwKAcsuwsnDs-CI1kruMH.webp",l="/assets/Yga5bKDucoCAI6xMV7ucwq8HnQc-Bh9MyLkW.png",k="/assets/RMfOb6oRboZGV8xtkGoc4BlHnNh-CaIArZFk.webp",p="/assets/Ugq7bu6yIooIjyxRw6ccfYbbnTf-zVnutV2d.webp",r="/assets/FaCTb171Iohpzsxm3l1c5Fmlnmb-BzEHPWaz.webp",d="/assets/1280X1280(1)-CJL928K_.png",o="/assets/1280X1280%20(1)(1)-C_Kd0hLO.png",g="/assets/VBRIbrQegofXkxxRhficadiRnTg-DZqzcwct.png",c="/assets/Vci2bIikDoaIYZxdaoVcaYrAn7q-RoXHcxCV.png",A="/assets/O68AbVLXAoSIqmxkUoUco5hrnii-DuM1polJ.png",y="data:image/webp;base64,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",D="/assets/DHoQb8wuvoXmcBxzdlxcWcPwnuQ-xb7w3FgD.webp",E="/assets/GmKObFtXqoEKAaxheokctaL1nte-DDU4k354.png",m="/assets/ONKGbFPtNom1GBxbKFYce1KFnLb-YaFb4ELl.png",f="/assets/Xq5zbZPSwoxAfUxZ3mTcK5kAnfe-D6DibkVL.webp",u="/assets/download_image(1)-Be1p9Feb.png",C="/assets/M9oeblOUkokCYHxuzopczY83n58-GdGlHkaz.png",B="/assets/HEvpbodcEoUF29xJfipcPbS0nXg-32ctoBQp.webp",v="/assets/YNDbb8SBUodwT7xYrdIcUxJVnbQ-DsrRBq70.png",b="/assets/M4H4bkd4noGXduxQ2c9cHJB9n8g-DLNP4pmN.png",_="/assets/N6rHbkOcGoSTSMx3WascJbGwnLh-BL8kic17.png",x="/assets/D6GtbKsPloRCChx9fL1ccWi9n1e-BrpqD8Fr.webp",F="/assets/L0vUbOGLLosQaGxmkZScsL8gnTc-CVQQzVVp.png",T="/assets/DEoobR3MKokqrnx0z3pcfi8CnGe-BZ5unkm7.webp",z="/assets/GQvAb8XsxoRugTxojv1cMIsdnQg-C-rRhgx3.webp",q="/assets/image-D_DV6b4u.png",w="/assets/image-1-BWqgH4hE.png",S={};function K(M,s){return t(),n("div",null,s[0]||(s[0]=[a('<h2 id="_1-1-transformer-整体结构" tabindex="-1"><a class="header-anchor" href="#_1-1-transformer-整体结构"><span>1. 1.Transformer 整体结构</span></a></h2><p>首先介绍<strong> Transformer</strong> 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：</p><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer 的整体结构，左图Encoder和右图Decoder</p><div class="hint-container note"><p class="hint-container-title">可以看 到<strong> Transformer </strong>由<strong><code>Encoder</code></strong> 和<strong><code>Decoder</code></strong>两个部分组成，<strong><code>Encoder</code></strong> 和<strong><code>Decoder</code></strong> 都包含<strong> 6 个 <code>block</code>。</strong></p></div><ul><li><p><strong>Transformer </strong>的工作流程大体如下：</p><ul><li><strong>第一步</strong>：获取输入句子的每一个单词的表示<strong>向量 X</strong>，X由单词的<strong><code> Embedding</code></strong>（Embedding就是从原始数据提取出来的<strong><code>Feature</code></strong>） 和单词位置的<strong><code> Embedding</code></strong> 相加得到。</li></ul></li></ul><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer 的输入表示</p><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transofrmer Decoder 预测</p><div class="hint-container note"><p class="hint-container-title">上图<strong><code>Decoder</code></strong>接收了<strong><code> Encoder </code></strong>的<strong>编码矩阵 C</strong>，然后首先输入一个翻译开始符 <strong><code>&quot;&lt;Begin&gt;&quot;</code></strong>，预测第一个单词 <strong><code>&quot;I&quot;</code></strong>；然后输入翻译开始符<code></code><strong><code>&quot;&lt;Begin&gt;&quot;</code></strong> 和单词<strong><code> &quot;I&quot;</code></strong>，预测单词<strong><code> &quot;have&quot;</code></strong>，以此类推。这是 <strong>Transformer</strong> 使用时候的大致流程，接下来是里面各个部分的细节。</p></div><h2 id="_2-transformer-的输入" tabindex="-1"><a class="header-anchor" href="#_2-transformer-的输入"><span>2. Transformer 的输入</span></a></h2><div class="hint-container note"><p class="hint-container-title"><strong>Transformer</strong> 中单词的输入表示<strong> x</strong>由<strong><code>单词 Embedding</code></strong> 和<strong><code>位置 Embedding </code></strong> <strong>（<code>Positional Encoding</code>）</strong>相加得到。</p></div><figure><img src="'+k+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer 的输入表示</p><h3 id="_2-1-单词-embedding" tabindex="-1"><a class="header-anchor" href="#_2-1-单词-embedding"><span>2.1 单词 <code>Embedding</code></span></a></h3><p><strong><code>单词的 Embedding</code></strong>有很多种方式可以获取，例如可以采用 <strong>Word2Vec</strong>、<strong>Glove </strong>等算法预训练得到，也可以在 <strong>Transformer</strong> 中训练得到。</p><h3 id="_2-2-位置-embedding" tabindex="-1"><a class="header-anchor" href="#_2-2-位置-embedding"><span>2.2 位置 <code>Embedding</code></span></a></h3><div class="hint-container note"><p class="hint-container-title"><strong><code>Transformer</code></strong> 中除了单词的<strong> Embedding</strong>，还需要使用位置 Embedding 表示单词出现在句子中的位置。因为 Transformer <strong><code>不采用 RNN 的结构</code></strong>，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 <strong>NLP</strong> 来说非常重要。所以 <strong>Transformer</strong> 中使用位置 <strong>Embedding</strong> 保存单词在序列中的相对或绝对位置。</p></div><p>位置 Embedding 用<strong> PE</strong>表示，<strong>PE</strong> 的维度与单词 <strong><code>Embedding</code></strong> 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者</p><p><strong>计算公式如下：</strong></p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>其中，<strong><code>pos 表示单词在句子中的位置</code></strong>，<strong><code>d 表示 PE的维度 (与词 Embedding 一样</code>)</strong>，<strong><code>2i 表示偶数的维度</code></strong>，<strong><code>2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)</code></strong>。使用这种公式计算 PE 有以下的好处：</p><div class="hint-container note"><p class="hint-container-title">注</p><ul><li>使<strong> PE</strong> 能够适应比<strong>训练集</strong>里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的<strong> Embedding</strong>。</li><li><strong>可以让模型容易地计算出相对位置</strong>，对于固定长度的间距<strong> k，PE(pos+k) </strong>可以用 <strong>PE(pos) </strong>计算得到。</li><li>因为 <strong><code>Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)</code></strong>。</li></ul></div><h2 id="_3-self-attention-自注意力机制" tabindex="-1"><a class="header-anchor" href="#_3-self-attention-自注意力机制"><span>3. Self-Attention（自注意力机制）</span></a></h2><p><strong> Transformer 的内部结构图</strong></p><ul><li><p>左侧为 <strong>Encoder block</strong></p></li><li><p>右侧为 <strong>Decoder block</strong></p></li><li><p>红色圈中的部分为 <strong><code>Multi-Head Attention</code></strong>，是由多个 <strong><code>Self-Attention</code></strong>组成的</p></li></ul><ol><li><p>可以看到 <strong>Encoder block </strong>包含一个 <code>Multi-Head Attention</code></p></li><li><p>而 <strong>Decoder block </strong>包含两个<strong><code> Multi-Head Attentio</code>n </strong>(其中有一个用到 Masked)。</p></li><li><p><strong><code>Multi-Head Attention</code></strong>上方还包括一个<strong> Add &amp; Norm 层</strong></p></li><li><p><strong>Add </strong>表示残差连接<strong><code> (Residual Connection)</code></strong> 用于防止网络退化，</p></li><li><p><strong>Norm</strong> 表示 <strong><code>Layer Normalization</code></strong>，用于对每一层的激活值进行<strong>归一化</strong>。</p></li></ol><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>因为 <strong>Self-Attention</strong>是 Transformer 的重点，所以我们重点关注 <strong><code>Multi-Head Attention</code></strong> 以及<strong><code> Self-Attention</code></strong>，首先详细了解一下<strong><code> Self-Attention </code></strong>的内部逻辑。</p><h3 id="_3-1-self-attention-结构" tabindex="-1"><a class="header-anchor" href="#_3-1-self-attention-结构"><span>3.1 <code>Self-Attention 结构</code></span></a></h3><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Self-Attention 结构</p><p>上图是 <strong><code>Self-Attention</code></strong>的结构，在计算的时候需要用到矩阵<strong>Q(查询),K(键值)</strong>,<strong>V(值)</strong>。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而<strong>Q,K,V</strong>正是通过 <strong><code>Self-Attention</code></strong> 的输入进行<strong>线性变换</strong>得到的。</p><h3 id="_3-2-q-k-v-的计算" tabindex="-1"><a class="header-anchor" href="#_3-2-q-k-v-的计算"><span>3.2 <strong><code>Q, K, V 的计算</code></strong></span></a></h3><div class="hint-container note"><p class="hint-container-title"><strong><code>Self-Attention</code></strong>的输入用<strong>矩阵X</strong>进行表示，则可以使用<strong>线性变阵矩阵WQ</strong>,<strong>WK</strong>,<strong>WV</strong>计算得到<strong>Q</strong>,<strong>K</strong>,<strong>V</strong>。计算如下图所示，注意 X, Q, K, V 的每一行都表示一个单词。</p></div><figure><img src="'+o+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Q, K, V 的计算</p><h3 id="实现" tabindex="-1"><a class="header-anchor" href="#实现"><span><strong>实现</strong></span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># self-attention 实现</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> numpy </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">as</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> np</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> math </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">class</span><span style="--shiki-light:#2E8F82;--shiki-dark:#5DA994;"> Self_Attention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">Module</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # input : batch_size * seq_len * input_dim</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # q : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # k : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # v : batch_size * input_dim * dim_v</span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        super</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Self_Attention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">__init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">_norm_fact </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 1</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> /</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> forward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        Q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # Q: batch_size * seq_len * dim_k</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        K </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # K: batch_size * seq_len * dim_k</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        V </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # V: batch_size * seq_len * dim_v</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">        # Q * K.T() # batch_size * seq_len * seq_len</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        atten </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Softmax</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">            dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">bmm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> K</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">permute</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)))</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> *</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">_norm_fact</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">        # Q * K.T() * V # batch_size * seq_len * dim_v</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        output </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">bmm</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> V</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        return</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> output</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">randn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self_atten </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> Self_Attention</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 5</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # input_dim:2, k_dim:4, v_dim:5</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">res </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> self_atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">res</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">  # [4,3,5]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-3-self-attention-的输出" tabindex="-1"><a class="header-anchor" href="#_3-3-self-attention-的输出"><span>3.3 <code> Self-Attention 的输出</code></span></a></h3><p>得到矩阵 <strong>Q</strong>, <strong>K</strong>, <strong>V</strong>之后就可以计算出<strong><code> Self-Attention </code></strong>的输出了，计算的公式如下：</p><figure><img src="`+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Self-Attention 的输出</p><div class="hint-container note"><p class="hint-container-title">公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以 $$d_k$$的平方根。<strong>Q乘以K</strong>的转置后，得到的矩阵行列数都为 <strong>n</strong>，<strong>n </strong>为句子单词数，这个矩阵可以表示单词之间的<strong> attention</strong> 强度。下图为Q乘以$$K^{T}$$ ，<strong>1234</strong> 表示的是句子中的单词。</p></div><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Q乘以K的转置的计算</p><p>得到$$QK^{T}$$之后，使用 <strong>Softmax</strong> 计算每一个单词对于其他单词的 <strong>attention</strong> 系数，公式中的 <strong><code>Softmax</code></strong>是对矩阵的每一行进行 <strong><code>Softmax</code></strong>，即每一行的和都变为 1.</p><figure><img src="'+A+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>对矩阵的每一行进行 Softmax</p><p>得到 <strong><code>Softmax</code></strong> 矩阵之后可以和V相乘，得到最终的输出Z。</p><figure><img src="'+y+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Self-Attention 输出</p><p>上图中<strong><code>Softmax</code></strong>矩阵的第 1 行表示单词 1 与其他所有单词的<strong> attention </strong>系数，最终单词 1 的输出 $$Z_1$$ 等于所有单词 i 的值 $$V_i$$ 根据 <strong>attention</strong> 系数的比例加在一起得到，如下图所示：</p><figure><img src="'+D+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Zi 的计算方法</p><h3 id="_3-4-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_3-4-multi-head-attention"><span>3.4 <strong><code>Multi-Head Attention</code></strong></span></a></h3><p>在上一步，我们已经知道怎么通过 <strong><code>Self-Attention</code>计算得到输出矩阵 Z</strong>，而 <strong><code>Multi-Head Attention</code></strong> 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的<strong>结构图</strong>。</p><figure><img src="'+E+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Multi-Head Attention</p><p>从上图可以看到 <strong><code>Multi-Head Attention</code></strong> 包含多个<strong> Self-Attention</strong> 层，首先将输入<strong>X</strong>分别传递到 <strong>h</strong> 个不同的 Self-Attention 中，计算得到 <strong>h </strong>个输出矩阵<strong>Z</strong>。下图是<strong> h=8 </strong>时候的情况，此时会得到 8 个输出<strong>矩阵Z</strong>。</p><h3 id="实现-1" tabindex="-1"><a class="header-anchor" href="#实现-1"><span>实现</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">#%%</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># Muti-head Attention 机制的实现</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> math </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nn </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">as</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">class</span><span style="--shiki-light:#2E8F82;--shiki-dark:#5DA994;"> Self_Attention_Muti_Head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">Module</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # input : batch_size * seq_len * input_dim</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # q : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # k : batch_size * input_dim * dim_k</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;">    # v : batch_size * input_dim * dim_v</span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        super</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Self_Attention_Muti_Head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">__init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        assert</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">%</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nums_head </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">==</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 0</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        assert</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">%</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nums_head </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">==</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 0</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Linear</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">input_dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nums_head</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_k</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> dim_v</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">        self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">_norm_fact </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 1</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> /</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> sqrt</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    </span></span>
<span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">    def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> forward</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">):</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        Q </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">//</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        K </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">k</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_k </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">//</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        V </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">v</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">dim_v </span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">//</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nums_head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">        print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">size</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">())</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        atten </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> nn</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Softmax</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">dim</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">matmul</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Q</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">K</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">permute</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)))</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> # Q * K.T() # batch_size * seq_len * seq_len</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        output </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">matmul</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">V</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">[</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">],</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> # Q * K.T() * V # batch_size * seq_len * dim_v</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">        </span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">        return</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> output</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">    </span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">rand</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">3</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># %%</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">Self_Attention_Muti_Head</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">4</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">y</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">atten</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">x</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">y</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># %%</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>多个 Self-Attention</p><p>得到 <strong>8 个输出矩阵</strong> $$Z_1$$ 到 $$Z_8$$之后，<strong><code>Multi-Head Attention</code></strong>将它们拼接在一起<strong> (Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出Z。</p><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Multi-Head Attention 的输出</p><p>可以看到 <strong><code>Multi-Head Attention</code></strong> 输出的<strong>矩阵Z</strong>与其输入的<strong>矩阵X</strong>的维度是一样的。</p><h2 id="_4-encoder-结构" tabindex="-1"><a class="header-anchor" href="#_4-encoder-结构"><span>4. Encoder 结构</span></a></h2><p><strong>红色部分</strong>是 Transformer 的<strong><code> Encoder block</code></strong> 结构，可以看到是由 <strong><code>Multi-Head Attention</code></strong>, <strong><code>Add &amp; Norm</code></strong>, <strong><code>Feed Forward</code></strong>, <strong>Add &amp; Norm</strong> 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer Encoder block</p><h3 id="_4-1-add-norm" tabindex="-1"><a class="header-anchor" href="#_4-1-add-norm"><span>4.1 Add &amp; Norm</span></a></h3><p><strong>Add &amp; Norm </strong>层由 Add 和 Norm 两部分组成，其计算公式如下：</p><figure><img src="'+C+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Add &amp;amp;amp;amp;amp; Norm 公式</p><p>其中<strong> X</strong>表示 <strong><code>Multi-Head Attention</code></strong><code></code>或者 <strong>Feed Forward</strong> 的输入，<strong><code>MultiHeadAttention(X</code></strong>) 和 <strong><code>FeedForward(X) </code></strong>表示输出 (输出与输入<strong> X </strong>维度是一样的，所以可以相加)</p><p><strong>Add</strong>指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决<strong>多层网络训练</strong>的问题，可以让网络只关注当前差异的部分，在 <strong>ResNe</strong>t 中经常用到：</p><figure><img src="'+B+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>残差连接</p><p><strong>Norm</strong>指 <strong><code>Layer Normalization</code></strong>，通常用于 <strong>RNN</strong> 结构，<strong><code>Layer Normalization</code></strong> 会将每一层<strong>神经元</strong>的输入都转成<strong>均值方差</strong>都一样的，这样可以加快<strong>收敛</strong>。</p><h3 id="_4-2-feed-forward" tabindex="-1"><a class="header-anchor" href="#_4-2-feed-forward"><span>4.2 Feed Forward</span></a></h3><p><strong>Feed Forward </strong>层比较简单，是一个两层的<strong>全连接层</strong>，第一层的<strong>激活函数</strong>为<strong><code> Relu</code></strong>，第二层不使用激活函数，对应的公式如下。</p><figure><img src="'+v+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Feed Forward</p><p><strong>X</strong>是输入，<strong><code>Feed Forward </code></strong>最终得到的输出矩阵的维度与X一致。</p><h3 id="_4-3-组成-encoder" tabindex="-1"><a class="header-anchor" href="#_4-3-组成-encoder"><span>4.3 组成 Encoder</span></a></h3><div class="hint-container note"><p class="hint-container-title">通过上面描述的<strong><code> Multi-Head Attention</code>,<code> Feed Forward</code></strong><code>,</code><strong><code> Add &amp; Norm</code></strong> 就可以构造出一个 <strong>Encoder block</strong>，<strong>Encoder block </strong>接收输入矩阵 $$X_{(n\\times d)}$$ ，并输出一个矩阵$$O_{(n\\times d)}$$ 。通过多个 Encoder block 叠加就可以组成 Encoder。</p></div><ul><li><p>第一个 <strong><code>Encoder block </code></strong>的输入为句子单词的表示<strong>向量矩阵</strong>，</p></li><li><p>后续<strong><code>Encoder block</code></strong> 的输入是前一个 Encoder block 的<strong>输出</strong></p></li><li><p>最后一个 <strong><code>Encoder block</code></strong> 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到<strong> Decoder</strong> 中。</p></li></ul><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-decoder-结构" tabindex="-1"><a class="header-anchor" href="#_5-decoder-结构"><span>5. Decoder 结构</span></a></h2><p><strong>红色部分</strong>为 <strong>Transformer</strong> 的<strong> Decoder block </strong>结构，与 Encoder block 相似，但是存在一些区别：</p><ul><li><p>包含两个<strong><code> Multi-Head Attention</code></strong> 层。</p></li><li><p>第一个 <strong><code>Multi-Head Attention</code></strong> 层采用了 <strong>Masked</strong> 操作。</p></li><li><p>第二个<strong><code>Multi-Head Attention</code></strong> 层的<strong>K</strong>, <strong>V</strong>矩阵使用 <strong>Encoder</strong> 的编码信息矩阵C进行计算，而Q使用上一个 <strong>Decoder block</strong> 的输出计算。</p></li><li><p>最后有一个 <strong><code>Softmax</code></strong> 层计算下一个翻译单词的概率。</p></li></ul><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Transformer Decoder block</p><h3 id="_5-1-第一个-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_5-1-第一个-multi-head-attention"><span>5.1 <code>第一个 Multi-Head Attention</code></span></a></h3><div class="hint-container note"><p class="hint-container-title"><strong><code>Decoder block</code></strong> 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 &quot;我有一只猫&quot; 翻译成 &quot;I have a cat&quot; 为例，了解一下 Masked 操作。</p></div><ul><li>在<strong> Decoder</strong> 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 &quot;&lt;Begin&gt;&quot; 预测出第一个单词为<strong> &quot;I&quot;</strong>，然后根据输入<strong><code> &quot;&lt;Begin&gt; I</code>&quot;</strong> 预测下一个单词 <strong>&quot;<code>have&quot;</code>。</strong></li></ul><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Decoder 预测</p><ul><li><p><strong>Decoder </strong>可以在训练的过程中使用 <strong><code>Teacher Forcing</code></strong> 并且<strong>并行化训练</strong>，即将正确的单词序列<strong><code> (&lt;Begin&gt; I have a cat) </code></strong>和对应输出<strong><code> (I have a cat &lt;end&gt;)</code></strong>传递到<strong> Decoder</strong>。那么在预测第 <strong><code>i </code></strong>个输出时，就要将第<strong><code> i+1</code></strong> 之后的单词掩盖住，</p></li><li><p>注意 <strong>Mask</strong> 操作是在 <strong><code>Self-Attention</code></strong> 的<strong><code> Softmax</code></strong>之前使用的，下面用 0 1 2 3 4 5 分别表示<strong><code> &lt;Begin&gt; I have a cat &lt;end&gt;</code></strong>。</p></li></ul><p><strong>第一步</strong>：</p><div class="hint-container note"><p class="hint-container-title">1. 是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 &quot;&lt;Begin&gt; I have a cat&quot; (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p></div><figure><img src="'+F+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>输入矩阵与 Mask 矩阵</p><p><strong>第二步</strong>：</p><div class="hint-container note"><p class="hint-container-title">2. &#39;接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算Q和 $$K^T\\text{ 的乘积 }QK^T$$</p></div><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Q乘以K的转置</p><p><strong>第三步</strong>：</p><div class="hint-container note"><p class="hint-container-title">3. 在得到 $$QK^{T}$$之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p></div><figure><img src="'+z+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Softmax 之前 Mask</p><p>得到 Mask $$QK^{T}$$ 之后在 Mask $$QK^{T}$$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p><p><strong>第四步</strong>：</p><div class="hint-container note"><p class="hint-container-title">4. 使用 Mask $$QK^{T}$$与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 $$Z_1$$ 是只包含单词 1 信息的。</p></div><p><strong>第五步</strong>：</p><div class="hint-container note"><p class="hint-container-title">5. 通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $$Z_i$$ ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出$$Z_i$$ 然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。</p></div><h3 id="_5-2-第二个-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_5-2-第二个-multi-head-attention"><span>5.2 <code> 第二个 Multi-Head Attention</code></span></a></h3><ul><li><p><strong><code>Decoder block</code></strong> 第二个 <strong><code>Multi-Head Attention</code></strong> 变化不大， 主要的区别在于其中 <strong><code>Self-Attention </code></strong>的 <strong>K</strong>, <strong>V</strong>矩阵不是使用 上一个<strong><code>Decoder block</code></strong> 的输出计算的，而是使用<strong> Encoder</strong> 的编码信息矩阵 C 计算的。</p></li><li><p>根据 <strong>Encoder</strong> 的输出 <strong>C</strong>计算得到 <strong>K</strong>, <strong>V</strong>，根据上一个<strong><code> Decoder block</code></strong> 的输出<strong> Z </strong>计算 <strong>Q </strong>(如果是第一个<strong><code>Decoder block</code></strong><code></code>则使用输入<strong>矩阵 X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p></li><li><p>这样做的好处是在 <strong>Decoder </strong>的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p></li></ul><h3 id="_5-3-softmax预测输出单词" tabindex="-1"><a class="header-anchor" href="#_5-3-softmax预测输出单词"><span>5.3 <strong><code>Softmax</code></strong>预测输出单词</span></a></h3><p><strong>Decoder block </strong>最后的部分是利用 <strong><code>Softmax</code></strong> 预测下一个单词，在之前的<strong>网络层</strong>我们可以得到一个最终的输出 Z，因为 <strong>Mask</strong> 的存在，使得单词 <strong><code>0</code></strong>的输出 <strong><code>Z0</code></strong> 只包含单词 <strong><code>0</code></strong>的信息，如下：</p><figure><img src="'+q+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Decoder Softmax 之前的 Z</p><p><strong><code>Softmax</code></strong> 根据输出矩阵的每一行预测下一个单词：</p><figure><img src="'+w+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Decoder Softmax 预测</p><p>这就是 <strong><code>Decoder block</code></strong> 的定义，与<strong><code>Encoder</code></strong>一样，<strong><code>Decoder</code></strong> 是由多个<strong><code> Decoder block</code></strong> 组合而成。</p><h2 id="_6-transformer-总结" tabindex="-1"><a class="header-anchor" href="#_6-transformer-总结"><span>6. Transformer 总结</span></a></h2><div class="hint-container tip"><p class="hint-container-title">提示</p><ul><li>Transformer 与 RNN 不同，可以比较好地并行训练。</li><li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li><li>Transformer 的重点是 Self-Attention 结构，其中用到的 Q, K, V矩阵通过输出进行线性变换得到。</li><li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li></ul></div>',131)]))}const N=i(S,[["render",K]]),Q=JSON.parse('{"path":"/ai/%E9%80%90%E5%B1%82%E5%88%86%E8%A7%A3Transformer/","title":"逐层分解Transformer","lang":"zh-CN","frontmatter":{"title":"逐层分解Transformer","createTime":"2025/07/07 09:44:44","permalink":"/ai/逐层分解Transformer/","description":"1. 1.Transformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： Transformer 的整体结构，左图Encoder和右图Decoder 可以看 到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和Decoder 都包...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"逐层分解Transformer\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-07-07T01:58:48.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://docs.sreio.com/ai/%E9%80%90%E5%B1%82%E5%88%86%E8%A7%A3Transformer/"}],["meta",{"property":"og:site_name","content":"Sreio Docs"}],["meta",{"property":"og:title","content":"逐层分解Transformer"}],["meta",{"property":"og:description","content":"1. 1.Transformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： Transformer 的整体结构，左图Encoder和右图Decoder 可以看 到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和Decoder 都包..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-07T01:58:48.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-07T01:58:48.000Z"}]]},"readingTime":{"minutes":11.9,"words":3570},"git":{"createdTime":1751853528000,"updatedTime":1751853528000,"contributors":[{"name":"sreio","username":"sreio","email":"ingwei@163.com","commits":1,"avatar":"https://avatars.githubusercontent.com/sreio?v=4","url":"https://github.com/sreio"}],"changelog":[{"hash":"61046e09f4c875c19d26a59c6758496fd3f8b2c0","time":1751853528000,"email":"ingwei@163.com","author":"sreio","message":"ai &#x26; sites"}]},"autoDesc":true,"filePathRelative":"ai/1.逐层分解Transformer.md","headers":[]}');export{N as comp,Q as data};
